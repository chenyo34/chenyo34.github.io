Regularized Linear Regression  
Penatly Function: 
$$
\begin{aligned}
\mathcal R(w) &= \alpha ||w||_{1} + \frac{\beta}{2}||w||_2\\
&= \sum^D_{i=1} \alpha_i |w_i| + \frac{1}{2}\sum^D_{j=1} \beta_i w_j^2\\
&= \alpha |w| + \frac{1}{2}\beta w^Tw,
\end{aligned}
$$ 
where $\alpha = \{\alpha_1, ..., \alpha_d\}$ and $ \beta = \{\beta_1, ..., \beta_d\}$.

* If $\alpha = \beta = \vec{0}$, it is ordinary Linear Regression
* If $\alpha = \vec{0}$, it is Ridge Regression. 
* IF $\beta =  \vec{0}$, it is LASSO Regression.


$$
\begin{aligned}
\mathcal{L}^{\alpha\beta}_{reg} &= \mathcal{L}(X,Y) + \mathcal{R}(w) \\
\end{aligned}
$$

Gradient Desent
$$
\begin{aligned}
\frac{\partial}{\partial w_k}R(w) &= \alpha_k \frac{\partial}{\partial w_k} |w_{k}| + \frac{1}{2}\beta_k \frac{\partial}{\partial w_k} w_k^2\\
&= 
    \begin{cases}
    \alpha_k + \beta_kw_k, & \text{if}\ w_k > 0\\
    -\alpha_k + \beta_kw_k, & \text{if}\ w_k < 0\\
    \beta_kw_k, & \text{if}\ w_k = 0
    \end{cases}\\

\frac{\partial}{\partial w_k}\mathcal{L}(X,Y) &= \frac{1}{2N}\frac{\partial}{\partial w_k} \sum^N_{n=1}(\hat{y}^{(n)} - y^{(n)})^2\\
&= \frac{1}{2N} \frac{\partial}{\partial w_k} \sum^N_{n=1}(\sum^D_{i=1}w_ix^{(n)}_i + b - y^{(n)})^2\\
&= \frac{1}{2N} \sum^N_{n=1}2(\sum^D_{i=1}w_ix^{(n)}_i + b - y^{(n)})x_k^{(n)}\\
&= \frac{1}{N} \sum^N_{n=1}(\sum^D_{i=1}w_ix^{(n)}_i + b - y^{(n)})x_k^{(n)}
\end{aligned}
$$

* If the bias term is dropped, then:
$$
\begin{aligned}
\frac{\partial}{\partial w_k}\mathcal{L}(X,Y) 
&= \frac{1}{N} \sum^N_{n=1}(\sum^D_{i=1}w_ix^{(n)}_i - y^{(n)})x_k^{(n)}+\beta_kw_k\\
&= \frac{1}{N} \sum^N_{n=1}\sum^D_{i=1}w_ix^{(n)}_ix_k^{(n)} - \frac{1}{N}\sum^N_{n=1}y^{(n)}x^{(n)}_k+\beta_kw_k\\
&=  \sum^D_{i=1} \frac{1}{N} \sum^N_{n=1} w_i\Big(x^{(n)}_ix_k^{(n)} + \beta_k I(i =k)\Big) - \frac{1}{N}\sum^N_{n=1}y^{(n)}x^{(n)}_k
\end{aligned}
$$

Deinfe $X_{n\times d} = [x^{(1)}, \dots, x^{(n)}]^T$, 
$Y_{n\times 1} = [y^{(1)}, \dots, y^{(n)}]$ and $w_{d\times 1}=[w_1, \dots, w_d]^T$

$$
\begin{aligned}
\nabla \mathcal{L}(X,Y) &= \frac{\partial}{\partial w}(w^TX^T - Y^T)(Xw-Y) + \beta I ww^T\\
&= \frac{\partial}{\partial w}-w^TX^TY + w^TX^TXw + Y^TY - Y^TXw + \beta I w^Tw\\
&= \frac{\partial}{\partial w}-w^TX^TY + w^TX^TXw + Y^TY - Y^TXw + \beta I w^Tw\\
& \text{Since}\ (w^TX^TY)_{1\times 1} = (Y^TXw)_{1\times 1}\\
&= \frac{\partial}{\partial w} Y^TY - 2w^TX^TY  + w^TX^TXw+ \beta Iw^Tw\\
&= 0 - 2X^TY - 2X^TXw + 2\beta Iw\\
&= 2w(X^TX + \beta I) - 2X^TY\\
\hat{w} &= \argmin \mathcal{L}(X,Y)\\
&= (X^TX + \beta I)^{-1}X^TY
\end{aligned} 
$$